# CH1-BERT简介

==BERT(Bidirectional Encoder Representations From Transformers)模型使用预训练和微调的方式来完成自然语言处理任务.这些任务包括包括问答系统、情感分析和语言推理等.==

BERT模型代指本来的BERT模型和BERTology系列模型.





BERT模型所适用的主要场景.

+ 处理类似阅读理解的任务.
+ 处理句子与段落间的匹配任务.
+ 提取句子深层语义特征的任务.
+ 基于句子或段落级别的短文本(长度小于512个子词的文本)处理任务.



BERT模型是由“Transformer模型中的编码器(Encoder)+双向(正向序列和反向序列)结构”组成.